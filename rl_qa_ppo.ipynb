{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library for data processing\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import re\n",
    "import itertools\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "import accelerate\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, AdamW, get_scheduler, GPTNeoModel, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "import openai\n",
    "\n",
    "#from torchrl.data import PrioritizedReplayBuffer, ReplayBuffer\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pickle\n",
    "import gym\n",
    "\n",
    "base_dir=\"<YOUR PATH>\"\n",
    "accelerator=accelerate.Accelerator()\n",
    "device=accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e46085368c140d6a6d5e01c2f4bb4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8eaf905e0dd42f6a262e89e557587b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/5.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bfa78cce044fadaced2fadeca4800f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset hotpot_qa/fullwiki to /Users/hyungmoonko/.cache/huggingface/datasets/hotpot_qa/fullwiki/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db579dc0b084ca4870ea6b73ea5ecd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61f3743f7d045319d55c1b8b1f51bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/566M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b1a03462254bdba5763d648db1f679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/47.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8ce1617ab549abaf2cb8a9f3606a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/46.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2bb3c68fe3c437a937c68d777ff2427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/90447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16056d245d0a44efa42ad8fbefbf7fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/7405 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e233d9d65a54a33a811a9b09e1c6b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7405 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset hotpot_qa downloaded and prepared to /Users/hyungmoonko/.cache/huggingface/datasets/hotpot_qa/fullwiki/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset hotpot_qa/distractor to /Users/hyungmoonko/.cache/huggingface/datasets/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810df4eba9a8457ba50723f762c1ff06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bcde97247f24dac9d1af0cf704d870f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/46.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fe9d582dc049f3be58177b7a4d1d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/90447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50568c274a15489c8f7184921772b197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/7405 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset hotpot_qa downloaded and prepared to /Users/hyungmoonko/.cache/huggingface/datasets/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hotpot_qa (/Users/hyungmoonko/.cache/huggingface/datasets/hotpot_qa/fullwiki/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5)\n"
     ]
    }
   ],
   "source": [
    "hotpot_train= load_dataset(\"hotpot_qa\", name=\"fullwiki\", split=\"train\")\n",
    "hotpot_val = load_dataset(\"hotpot_qa\", name=\"distractor\", split=\"validation\")\n",
    "hotpot_test= load_dataset(\"hotpot_qa\", name=\"fullwiki\", split=\"test\")\n",
    "\n",
    "#hotpot_train.save_to_disk(os.path.join(base_dir, 'data/hotpot_train_raw'))\n",
    "#hotpot_val.save_to_disk(os.path.join(base_dir, 'data/hotpot_val_raw'))\n",
    "#hotpot_test.save_to_disk(os.path.join(base_dir, 'data/hotpot_test_raw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_name=\"multi-qa-MiniLM-L6-dot-v1\"\n",
    "n_layers=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment for whole hotpot data ex) for hotpot train\n",
    "\n",
    "#for real data: use format of {question, context} => \n",
    "#context={titles=[title1, title2,...], sentences=[[doc1_s1, doc1_s2,..], [doc2_s1, doc2_s2...], ..]}\n",
    "\n",
    "class HotPotQAEnv():\n",
    "  def __init__(self, hotpot_dataset, train):\n",
    "    self.train=train\n",
    "    self.hotpot_dataset=hotpot_dataset\n",
    "    #tokenizer for early termination due to context window limit.\n",
    "    self.tokenizer = transformers.AutoTokenizer.from_pretrained(\"sentence-transformers/{:s}\".format(sbert_name))\n",
    "\n",
    "    #single data environment\n",
    "    #state variables\n",
    "    self.question=None\n",
    "    #passage has form of {'title': title, 'passage': passage}\n",
    "    self.labels=None\n",
    "    self.unselected_passages=None\n",
    "    self.selected_passages=None\n",
    "    self.accuracy=0\n",
    "  \n",
    "    #batch environment\n",
    "    self.horizon=0\n",
    "    self.batch_traj_step=0\n",
    "    self.batch_questions=None\n",
    "    self.batch_labels=None\n",
    "    self.batch_unselected_passages=None\n",
    "    self.batch_selected_passages=None\n",
    "\n",
    "    #values for adaptive reward scheme\n",
    "    self.batch_positive_rewards=None\n",
    "    self.batch_negative_rewards=None\n",
    "\n",
    "    self.batch_accuracies=None\n",
    "\n",
    "    #normalizing(scaling) rewards\n",
    "    self.norm=5\n",
    "\n",
    "  def process_context(self, hotpot_data):\n",
    "    #context holds candidate passage data.\n",
    "    context=hotpot_data['context']\n",
    "\n",
    "    passages=[]\n",
    "    titles=context['title']\n",
    "    sentences_list=context['sentences']\n",
    "    #decompose document into every sentence => concept of taking many small steps\n",
    "    passage_to_idx={}\n",
    "    count=0\n",
    "    for idx, title in enumerate(titles):\n",
    "      #many sentences for a single document => multiple documents for one question.\n",
    "      for sent_id, sentence in enumerate(sentences_list[idx]):\n",
    "        passage={\n",
    "            'index': count,\n",
    "            'title': title,\n",
    "            'passage': sentence\n",
    "        }\n",
    "        passages.append(passage)\n",
    "        passage_to_idx[(title, sent_id)]=count\n",
    "        count+=1\n",
    "\n",
    "    #gold labels also present for eval and test sets\n",
    "    labels=[]\n",
    "    supp_titles=hotpot_data['supporting_facts']['title']\n",
    "    supp_sent_ids=hotpot_data['supporting_facts']['sent_id']\n",
    "    for supp_idx, supp_title in enumerate(supp_titles):\n",
    "      supp_sent_id=supp_sent_ids[supp_idx]\n",
    "      #there are some faulty data\n",
    "      if (supp_title, supp_sent_id) in passage_to_idx.keys():\n",
    "        label=passage_to_idx[(supp_title, supp_sent_id)]\n",
    "        labels.append(label)\n",
    "    return passages, labels\n",
    "    \n",
    "  def get_state_prompt(self, question, selected_passages):\n",
    "    #for computing state value\n",
    "    prompt=\"Question: \"+question\n",
    "    state_prompt=prompt+'\\n\\nSelected Passages:\\n'\n",
    "    #state prompt combines question and selected passages\n",
    "    if len(selected_passages)==0:\n",
    "      state_prompt+=\"Not Selected\\n\\n\"\n",
    "    for selected_passage in selected_passages:\n",
    "      title=selected_passage['title']\n",
    "      passage=selected_passage['passage']\n",
    "      state_prompt+=(\"Document Title: \"+title+\"\\nPassage: \"+passage+'\\n\\n')\n",
    "    return state_prompt\n",
    "  \n",
    "  def get_action_prompts(self, question, unselected_passages):\n",
    "    prompt=\"Question: \"+question\n",
    "    #has one prompt for each unselected passage\n",
    "    action_prompts=[]\n",
    "    for unselected_passage in unselected_passages:\n",
    "      action_prompt=prompt+\"\\n\\nSelected Passage:\\n\"\n",
    "      title=unselected_passage['title']\n",
    "      passage=unselected_passage['passage']\n",
    "      action_prompt+=(\"Document Title: \"+title+\"\\nPassage: \"+passage)\n",
    "      action_prompts.append(action_prompt)\n",
    "    return action_prompts\n",
    "  \n",
    "  def get_actor_prompt(self):\n",
    "    state_prompt=self.get_state_prompt(self.question, self.unselected_passages)\n",
    "    action_prompts=self.get_action_prompts(self.question, self.unselected_passages)\n",
    "    return state_prompt, action_prompts\n",
    "  \n",
    "  def reset(self):\n",
    "    #get random hotpot data\n",
    "    random_idx=np.random.randint(low=0, high=len(self.hotpot_dataset))\n",
    "    hotpot_data=self.hotpot_dataset[random_idx]\n",
    "    self.question=hotpot_data['question']\n",
    "    self.unselected_passages, self.labels=self.process_context(hotpot_data)\n",
    "    self.selected_passages=[]\n",
    "    self.accuracy=0\n",
    "    state=self.get_state()\n",
    "    return state\n",
    "  \n",
    "  def get_state(self):\n",
    "    q=self.question\n",
    "    sp=self.selected_passages[:]\n",
    "    usp=self.unselected_passages[:]\n",
    "    state={\n",
    "        'question': q,\n",
    "        'selected_passages': sp,\n",
    "        'unselected_passages': usp\n",
    "    }\n",
    "    return state\n",
    "  \n",
    "  #action idx refers to relative index within unselected passages. 0 (stop selecting) will always remain in index 0.\n",
    "  def step(self, action_idx):\n",
    "    #if you run out of unselected passages: recieve reward of automatically answer at this step\n",
    "    if len(self.unselected_passages)==0:\n",
    "      action_idx=0\n",
    "    \n",
    "    #answering action\n",
    "    if action_idx==0:\n",
    "      #termination: agent considers that the seleted passages are sufficient\n",
    "      termin_signal=True\n",
    "      reward, accuracy=self.give_final_reward(self.selected_passages, self.labels)\n",
    "      self.accuracy=accuracy\n",
    "    #choosing another passage\n",
    "    else:\n",
    "      #keep selecting action => # of available actions at every state: # of unselected passages+1\n",
    "      termin_signal=False\n",
    "      reward=-0.1\n",
    "    #move selected action to selected passages\n",
    "    selected_passage=self.unselected_passages.pop(action_idx)\n",
    "    self.selected_passages.append(selected_passage)\n",
    "    state_f=self.get_state()\n",
    "    return reward, state_f, termin_signal\n",
    "  \n",
    "  def get_accuracy(self, selected_passages, labels):\n",
    "    #give final reward based on how many labels they got correct. + should select no more\n",
    "    len_sp=len(selected_passages)\n",
    "    len_labels=len(labels)\n",
    "    count=0\n",
    "    #count correctly selected passages\n",
    "    for sp in selected_passages:\n",
    "      if sp['index'] in labels:\n",
    "        count+=1\n",
    "    accuracy=count/len_sp\n",
    "    return count, accuracy\n",
    "  \n",
    "  def get_batch_actor_prompts(self, mask):\n",
    "    batch_state_prompts=[]\n",
    "    batch_action_prompts_list=[]\n",
    "    for idx, question in enumerate(self.batch_questions):\n",
    "      masked=mask[idx]\n",
    "      #get only for the environments s.t. mask==True (episode not terminated)\n",
    "      if masked==False:\n",
    "        state_prompt=self.get_state_prompt(question, self.batch_selected_passages[idx])\n",
    "        action_prompts=self.get_action_prompts(question, self.batch_unselected_passages[idx])\n",
    "        batch_state_prompts.append(state_prompt)\n",
    "        batch_action_prompts_list.append(action_prompts)\n",
    "    return batch_state_prompts, batch_action_prompts_list\n",
    "  \n",
    "  def get_batch_states(self):\n",
    "    #return list of states\n",
    "    batch_states=[]\n",
    "    for idx, question in enumerate(self.batch_questions):\n",
    "      state={\n",
    "          'question': question,\n",
    "          'unselected_passages': self.batch_unselected_passages[idx][:],\n",
    "          'selected_passages': self.batch_selected_passages[idx][:]\n",
    "      }\n",
    "      batch_states.append(state)\n",
    "    return batch_states\n",
    "  \n",
    "  def batch_reset(self, batch_indices, horizon):\n",
    "    #horizon: max trajectory length\n",
    "    self.horizon=horizon\n",
    "    self.batch_traj_step=0\n",
    "    batch_size=len(batch_indices)\n",
    "    batch_hotpot_data=self.hotpot_dataset[batch_indices]\n",
    "    self.batch_questions=batch_hotpot_data['question']\n",
    "    contexts=batch_hotpot_data['context']\n",
    "    spfs=batch_hotpot_data['supporting_facts']\n",
    "    #get passages data\n",
    "    batch_usp=[]\n",
    "    batch_sp=[]\n",
    "    batch_labels=[]\n",
    "    batch_pos_ars=[]\n",
    "    batch_neg_ars=[]\n",
    "    #adaptive reward scheme\n",
    "    for idx, context in enumerate(contexts):\n",
    "      spf=spfs[idx]\n",
    "      #process context as in single env. step\n",
    "      passages, labels=self.process_context({'context': context, 'supporting_facts': spf})\n",
    "      batch_sp.append([])\n",
    "      batch_usp.append(passages)\n",
    "      batch_labels.append(labels)\n",
    "      len_labels=len(labels)\n",
    "      len_psgs=len(passages)\n",
    "      pos_reward=1\n",
    "      neg_reward=-0.2\n",
    "      #optimal_return=optimal_return_ftn(len_psgs, len_labels)\n",
    "      #pos_reward=(1/self.norm)*optimal_return/(len_labels+1)\n",
    "      #neg_reward=-(1/self.norm)*len_labels*(len_psgs+1)*optimal_return/(len_psgs*(len_labels+1)*(len_psgs-len_labels))\n",
    "      batch_pos_ars.append(pos_reward)\n",
    "      batch_neg_ars.append(neg_reward)\n",
    "    self.batch_positive_rewards=batch_pos_ars\n",
    "    self.batch_negative_rewards=batch_neg_ars\n",
    "    #print(batch_ars)\n",
    "    self.batch_unselected_passages=batch_usp\n",
    "    self.batch_selected_passages=batch_sp\n",
    "    self.batch_labels=batch_labels\n",
    "    #return batch states\n",
    "    batch_states=self.get_batch_states()\n",
    "\n",
    "    #reset accuracy\n",
    "    self.batch_accuracies=[0 for _ in range(batch_size)]\n",
    "    return batch_states\n",
    "  \n",
    "  def batch_step(self, batch_size, action_indices, mask):\n",
    "    assert self.batch_questions!=None, \"Run batch_reset() first to use step\"\n",
    "    self.batch_traj_step+=1 #increment trajectory step\n",
    "\n",
    "    batch_rewards=[]\n",
    "    batch_termin_signals=[]\n",
    "    batch_accs=[]\n",
    "    unpack_idx=0\n",
    "    for idx, unselected_passages in enumerate(self.batch_unselected_passages):\n",
    "      #take a environment step if episode is running\n",
    "      if mask[idx]==False:\n",
    "        selected_passages=self.batch_selected_passages[idx]\n",
    "        #relative index w.r.t currently unselected passages\n",
    "        action_idx=action_indices[unpack_idx]\n",
    "        unpack_idx+=1\n",
    "        #get action passage\n",
    "        action_psg=unselected_passages.pop(action_idx)\n",
    "        selected_passages.append(action_psg)\n",
    "        #updating state\n",
    "        self.batch_unselected_passages[idx]=unselected_passages\n",
    "        self.batch_selected_passages[idx]=selected_passages\n",
    "\n",
    "        #check for accuracy and correct count\n",
    "        correct_count, accuracy=self.get_accuracy(selected_passages, self.batch_labels[idx])\n",
    "        n_labels=len(self.batch_labels[idx])\n",
    "        #check exceeding context window\n",
    "        state_prompt=self.get_state_prompt(self.batch_questions[idx][:], self.batch_selected_passages[idx][:])\n",
    "        n_tokens=self.tokenizer(state_prompt, return_tensors='pt').input_ids.size(1)\n",
    "        #terminate if all correct psg.s are found or horizon is reached.\n",
    "        if correct_count==n_labels:\n",
    "          self.batch_accuracies[idx]=accuracy\n",
    "          #termination reward => should be positive only if everything is correct\n",
    "          termin_signal=True\n",
    "          reward=self.batch_positive_rewards[idx]\n",
    "        elif self.batch_traj_step==self.horizon or n_tokens>512:\n",
    "          self.batch_accuracies[idx]=accuracy\n",
    "          #termination reward => should be positive only if everything is correct\n",
    "          termin_signal=True\n",
    "          if accuracy==1:\n",
    "            reward=self.batch_positive_rewards[idx]\n",
    "          else:\n",
    "            reward=self.batch_negative_rewards[idx]\n",
    "        else:\n",
    "          termin_signal=False\n",
    "          if action_psg['index'] in self.batch_labels[idx]:\n",
    "            reward=self.batch_positive_rewards[idx] #large adaptive reward for correct answer\n",
    "          else:\n",
    "            reward=self.batch_negative_rewards[idx] #negative reward for wrong answer.\n",
    "      #for terminated episode: give reward of 0 and keep termin signal as True => defined as a padding episode step.\n",
    "      else:\n",
    "        #state remains the same for masked trajectories\n",
    "        reward=0\n",
    "        termin_signal=True\n",
    "      batch_rewards.append(reward)\n",
    "      batch_termin_signals.append(termin_signal)\n",
    "    batch_state_fs=self.get_batch_states()\n",
    "    return batch_rewards, batch_state_fs, batch_termin_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateValueModule(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(StateValueModule, self).__init__()\n",
    "    #state value module takes question + selected passages as input\n",
    "    #state value module estmiates the future return not given the available actions.\n",
    "    #quality of available actions (passages) is the role of retriever.\n",
    "    self.tokenizer = transformers.AutoTokenizer.from_pretrained(\"sentence-transformers/{:s}\".format(sbert_name))\n",
    "    self.model = transformers.AutoModel.from_pretrained(\"sentence-transformers/{:s}\".format(sbert_name)).to(device)\n",
    "    layers=self.model.encoder.layer\n",
    "    new_layers=layers[:n_layers]\n",
    "    self.model.encoder.layer=new_layers.to(device)\n",
    "    self.head1=nn.Linear(384,128).to(device)\n",
    "    self.head2=nn.Linear(128,1).to(device)\n",
    "\n",
    "    self.layernorm=nn.LayerNorm(128).to(device)\n",
    "    self.element_init()\n",
    "  \n",
    "  def element_init(self):\n",
    "    nn.init.kaiming_normal_(self.head1.weight)\n",
    "    nn.init.zeros_(self.head1.bias)\n",
    "    nn.init.kaiming_normal_(self.head2.weight)\n",
    "    nn.init.zeros_(self.head2.bias)\n",
    "    return\n",
    "  \n",
    "  def forward(self, batch_state_prompts):\n",
    "    #module designed for returning batch of state values\n",
    "    relu=nn.ReLU()\n",
    "    dropout=nn.Dropout(p=0.2).to(device)\n",
    "    mlp=nn.Sequential(self.head1, self.layernorm, relu, dropout, self.head2)\n",
    "\n",
    "    #forward pass\n",
    "    tokenized= self.tokenizer(batch_state_prompts, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids, attention_mask, token_type_ids=tokenized.input_ids.to(device), tokenized.attention_mask.to(device), tokenized.token_type_ids.to(device)\n",
    "    model_output=self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=True)\n",
    "    embeddings=model_output.pooler_output\n",
    "    norms = torch.norm(embeddings, dim=1, keepdim=True)\n",
    "    normalized_embeddings=embeddings/norms\n",
    "    state_values=mlp(normalized_embeddings).squeeze(dim=1)\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLQA_Actor(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(RLQA_Actor, self).__init__()\n",
    "    self.tokenizer = transformers.AutoTokenizer.from_pretrained(\"sentence-transformers/{:s}\".format(sbert_name))\n",
    "    self.state_embedder = transformers.AutoModel.from_pretrained(\"sentence-transformers/{:s}\".format(sbert_name)).to(device)\n",
    "    layers=self.state_embedder.encoder.layer\n",
    "    new_layers=layers[:n_layers]\n",
    "    self.state_embedder.encoder.layer=new_layers.to(device)\n",
    "    #available actions=unselected passages\n",
    "    self.action_embedder=transformers.AutoModel.from_pretrained(\"sentence-transformers/{:s}\".format(sbert_name)).to(device)\n",
    "    layers=self.action_embedder.encoder.layer\n",
    "    new_layers=layers[:n_layers]\n",
    "    self.action_embedder.encoder.layer=new_layers.to(device)\n",
    "\n",
    "    #alternate: change to a single embedder+ couldn't find pretrained model smaller than multi-qa-MiniLM-L6-dot-v1\n",
    "    #self.embedder=transformers.AutoModel.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-dot-v1\").to(device)\n",
    "\n",
    "  #unit operations\n",
    "  def get_single_action_prob(self, state_prompt, action_prompts, action_idx, temperature=1):\n",
    "    categorical=self.single_forward(state_prompt, action_prompts, temperature)\n",
    "    action_prob=categorical.probs[action_idx]\n",
    "    return action_prob\n",
    "  \n",
    "  def single_forward(self, state_prompt, action_prompts, temperature=1):\n",
    "    state_embedding=self.state_embedder.encode(state_prompt, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    action_embeddings=self.action_embedder.encode(action_prompts, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "    logits=state_embedding.matmul(action_embeddings.permute(1,0))\n",
    "    if temperature==0:\n",
    "      argmax_idx=torch.argmax(logits, dim=-1)\n",
    "      new_probs=torch.zeros(logits).to(device)\n",
    "      new_probs[argmax_idx]=0\n",
    "      categorical=torch.distributions.categorical.Categorical(probs=new_probs)\n",
    "    else:\n",
    "      categorical=torch.distributions.categorical.Categorical(logits=logits)\n",
    "    return categorical\n",
    "\n",
    "  def get_action_probs(self, batch_state_prompts, batch_action_prompts_list, action_index_list, temperature=1):\n",
    "    #compute categoricals based on trajectory\n",
    "    categoricals=self.forward(batch_state_prompts, batch_action_prompts_list, temperature)\n",
    "    action_probs=[] #list of tensors\n",
    "    for idx, categorical in enumerate(categoricals):\n",
    "      action_prob=categorical.probs[action_index_list[idx]]\n",
    "      action_probs.append(action_prob.unsqueeze(dim=0))\n",
    "    #return as a single tensor.\n",
    "    action_probs_tensor=torch.cat(action_probs, dim=0)\n",
    "    return categoricals, action_probs_tensor\n",
    "    \n",
    "  #batch operations\n",
    "  def forward(self, batch_state_prompts, batch_action_prompts_list, temperature=1):\n",
    "    #input: multiple states => each state has multiple available actions (numbers aren't equal)\n",
    "    #concatenate all batch_action_prompts\n",
    "    batch_action_prompts_whole=[]\n",
    "    batch_action_intervals=[]\n",
    "    start_index=0\n",
    "    for baps in batch_action_prompts_list:\n",
    "      batch_action_prompts_whole.extend(baps)\n",
    "      start=start_index\n",
    "      end=start_index+len(baps)\n",
    "      batch_action_intervals.append([start, end])\n",
    "      start_index+=len(baps)\n",
    "    \n",
    "    #compute state embeddings\n",
    "    states_tokenized=self.tokenizer(batch_state_prompts, padding=True, truncation=True, return_tensors='pt')\n",
    "    states_input_ids, states_attention_mask, states_token_type_ids=states_tokenized.input_ids.to(device), states_tokenized.attention_mask.to(device), states_tokenized.token_type_ids.to(device)\n",
    "    state_embedder_output=self.state_embedder(input_ids=states_input_ids, attention_mask=states_attention_mask, token_type_ids=states_token_type_ids, return_dict=True)\n",
    "    state_embeddings=state_embedder_output.pooler_output\n",
    "    state_norms=torch.norm(state_embeddings, dim=1, keepdim=True)\n",
    "    normalized_state_embeddings=state_embeddings/state_norms\n",
    "\n",
    "    #compute action embeddings\n",
    "    actions_tokenized=self.tokenizer(batch_action_prompts_whole, padding=True, truncation=True, return_tensors='pt')\n",
    "    actions_input_ids, actions_attention_mask, actions_token_type_ids=actions_tokenized.input_ids.to(device), actions_tokenized.attention_mask.to(device), actions_tokenized.token_type_ids.to(device)\n",
    "    action_embedder_output=self.action_embedder(input_ids=actions_input_ids, attention_mask=actions_attention_mask, token_type_ids=actions_token_type_ids, return_dict=True)\n",
    "    action_embeddings_whole=action_embedder_output.pooler_output\n",
    "    action_norms=torch.norm(action_embeddings_whole, dim=1, keepdim=True)\n",
    "    normalized_action_embeddings_whole=action_embeddings_whole/action_norms\n",
    "    #torch.split doesn't retain gradients.\n",
    "    #action_embeddings_list=torch.split(action_embeddings_whole, batch_action_lengths, dim=0)\n",
    "\n",
    "    #generate categorical objects\n",
    "    categoricals=[]\n",
    "    for idx, state_embedding in enumerate(normalized_state_embeddings):\n",
    "      batch_action_interval=batch_action_intervals[idx]\n",
    "      action_embeddings=normalized_action_embeddings_whole[batch_action_interval[0]:batch_action_interval[1]]\n",
    "      logits=state_embeddings[idx].matmul(action_embeddings.permute(1,0))\n",
    "      #temperauter=0 => assigns prob 0 everywhere except argmax index\n",
    "      if temperature==0:\n",
    "        argmax_idx=torch.argmax(logits, dim=-1)\n",
    "        new_probs=torch.zeros(logits).to(device)\n",
    "        new_probs[argmax_idx]=1\n",
    "        categorical=torch.distributions.categorical.Categorical(probs=new_probs)\n",
    "      else:\n",
    "        categorical=torch.distributions.categorical.Categorical(logits=logits)\n",
    "      categoricals.append(categorical)\n",
    "    return categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLQA_Agent(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(RLQA_Agent, self).__init__()\n",
    "    self.actor=RLQA_Actor()\n",
    "    self.critic=StateValueModule() #used for GAE estimation.\n",
    "  \n",
    "  def load_actor(self, actor_name):\n",
    "    self.actor.load_state_dict(torch.load(os.path.join(base_dir, 'models/{:s}.pt'.format(actor_name))))\n",
    "    return\n",
    "  \n",
    "  def load_state_value_module(self, V_name):\n",
    "    self.V.load_state_dict(torch.load(os.path.join(base_dir, 'models/{:s}.pt'.format(V_name))))\n",
    "    return\n",
    "  \n",
    "  def single_forward(self, state_prompt, action_prompts, temperature=1):\n",
    "    if temperature==0: #greedy sampling\n",
    "      categorical=self.actor.single_forward(state_prompt, action_prompts, 1) #use 1 and argmax\n",
    "    else:\n",
    "      categorical=self.actor.single_forward(state_prompt, action_prompts, temperature)\n",
    "    \n",
    "    #take indices\n",
    "    if temperature==0:\n",
    "      #greedy sampling\n",
    "      action_idx=torch.argmax(categorical.logits, dim=-1)\n",
    "      action_prob=1\n",
    "    else:\n",
    "      action_idx=categorical.sample()\n",
    "      action_prob=categorical.probs[action_idx]\n",
    "    return action_idx, action_prob\n",
    "\n",
    "  #get action_indices & action_probs\n",
    "  def forward(self, batch_state_prompts, batch_action_prompts_list, temperature=1, no_grad=False):\n",
    "    action_indices=[]\n",
    "    action_probs=[]\n",
    "    #default: sampling w/o temperature\n",
    "    if temperature==0: #greedy sampling\n",
    "      if no_grad:\n",
    "        with torch.no_grad():\n",
    "          categoricals=self.actor(batch_state_prompts, batch_action_prompts_list, 1) #use 1 and argmax\n",
    "    else:\n",
    "      if no_grad:\n",
    "        with torch.no_grad():\n",
    "          categoricals=self.actor(batch_state_prompts, batch_action_prompts_list, temperature)\n",
    "    \n",
    "    #get action indices through sampling and get corres. probabilities\n",
    "    for categorical in categoricals:\n",
    "      action_idx=categorical.sample()\n",
    "      action_prob=categorical.probs[action_idx]\n",
    "      action_indices.append(action_idx)\n",
    "      action_probs.append(action_prob)\n",
    "    #action_probs_tensor=torch.cat([ap.unsqueeze(dim=0) for ap in action_probs], dim=0).to(device)\n",
    "    return categoricals, action_indices, action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for on-policy & off-policy training\n",
    "class BatchEpisodeSteps():\n",
    "  def __init__(self, states, action_indices, action_probs, rewards, state_fs, termin_signals, categoricals, masks):\n",
    "    #state => but \"state prompt\" only holds question and selected passages: used for state value\n",
    "    self.states=states\n",
    "    self.state_prompts, self.action_prompts_list=self.get_actor_prompts(self.states, masks)\n",
    "\n",
    "    #action related\n",
    "    self.action_indices=self.unpack_list(action_indices, masks) #relative to unselected passages\n",
    "    self.action_probs=self.unpack_list(action_probs, masks)\n",
    "\n",
    "    self.rewards=rewards\n",
    "\n",
    "    self.state_fs=state_fs\n",
    "    #termin_signals = new masks\n",
    "    self.state_f_prompts, self.action_f_prompts_list=self.get_actor_prompts(self.state_fs, termin_signals)\n",
    "\n",
    "    self.termin_signals=termin_signals\n",
    "\n",
    "    #used for kl-divergence computation.\n",
    "    self.categoricals=self.unpack_list(categoricals, masks)\n",
    "\n",
    "    self.masks=masks\n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    ep_step={\n",
    "        'state': self.states[idx],\n",
    "        'state_prompt': self.state_prompts[idx],\n",
    "        'action_prompts': self.action_prompts_list[idx],\n",
    "        'action_index': self.action_indices[idx],\n",
    "        'action_prob': self.action_probs[idx],\n",
    "        'reward': self.rewards[idx],\n",
    "        'state_f': self.state_fs[idx],\n",
    "        'state_f_prompt': self.state_f_prompts[idx],\n",
    "        'action_f_prompts': self.action_f_prompts_list[idx],\n",
    "        'termin_signal': self.termin_signals[idx],\n",
    "        'categorical': self.categoricals[idx],\n",
    "        'mask': self.masks[idx]\n",
    "    }\n",
    "    return ep_step\n",
    "  \n",
    "  #unpack to be same with mask length.\n",
    "  def unpack_list(self, props, mask):\n",
    "    unpacked_props=[None for _ in range(len(mask))]\n",
    "    props_idx=0\n",
    "    for m_idx, masked in enumerate(mask):\n",
    "      if not masked:\n",
    "        unpacked_props[m_idx]=props[props_idx]\n",
    "        props_idx+=1\n",
    "    return unpacked_props\n",
    "  \n",
    "  def unpack_tensor(self, tensor, mask):\n",
    "    unpacked_tensor=torch.zeros(len(mask)).to(device)\n",
    "    tensor_idx=0\n",
    "    for m_idx, masked in enumerate(mask):\n",
    "      if not masked:\n",
    "        unpacked_tensor[m_idx]=tensor[tensor_idx]\n",
    "        tensor_idx+=1\n",
    "    return unpacked_tensor\n",
    "  \n",
    "  def get_state_prompt(self, question, selected_passages):\n",
    "    #for computing state value\n",
    "    prompt=\"Question: \"+question\n",
    "    state_prompt=prompt+'\\n\\nSelected Passages:\\n'\n",
    "    #state prompt combines question and selected passages\n",
    "    if len(selected_passages)==0:\n",
    "      state_prompt+=\"Not Selected\\n\\n\"\n",
    "    for selected_passage in selected_passages:\n",
    "      title=selected_passage['title']\n",
    "      passage=selected_passage['passage']\n",
    "      state_prompt+=(\"Document Title: \"+title+\"\\nPassage: \"+passage+'\\n\\n')\n",
    "    return state_prompt\n",
    "  \n",
    "  def get_action_prompts(self, question, unselected_passages):\n",
    "    prompt=\"Question: \"+question\n",
    "    #has one prompt for each unselected passage\n",
    "    action_prompts=[]\n",
    "    for unselected_passage in unselected_passages:\n",
    "      action_prompt=prompt+\"\\n\\nSelected Passage:\\n\"\n",
    "      title=unselected_passage['title']\n",
    "      passage=unselected_passage['passage']\n",
    "      action_prompt+=(\"Document Title: \"+title+\"\\nPassage: \"+passage)\n",
    "      action_prompts.append(action_prompt)\n",
    "    return action_prompts\n",
    "    \n",
    "  def get_actor_prompts(self, states, mask):\n",
    "    state_prompts=[]\n",
    "    action_prompts_list=[]\n",
    "    for idx, state in enumerate(states):\n",
    "      state_prompt=self.get_state_prompt(state['question'], state['selected_passages'])\n",
    "      action_prompts=self.get_action_prompts(state['question'], state['unselected_passages'])\n",
    "      state_prompts.append(state_prompt)\n",
    "      action_prompts_list.append(action_prompts)\n",
    "    return state_prompts, action_prompts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory():\n",
    "  def __init__(self):\n",
    "    self.states=[]\n",
    "    self.state_prompts=[]\n",
    "    self.action_prompts_list=[]\n",
    "    self.action_indices=[]\n",
    "    self.action_probs=[]\n",
    "    self.rewards=[]\n",
    "    self.state_fs=[]\n",
    "    self.state_f_prompts=[]\n",
    "    self.action_f_prompts_list=[]\n",
    "    self.termin_signals=[]\n",
    "    self.categoricals=[]\n",
    "    self.masks=[]\n",
    "\n",
    "    self.finished=False\n",
    "\n",
    "    self.len_traj=0\n",
    "  \n",
    "  def get_discounted_cumsum_matrix(self, N, discount_rate):\n",
    "    dcm=torch.eye(N).to(device)\n",
    "    for offset in range(1,N):\n",
    "      max_row=N-offset\n",
    "      for row in range(max_row):\n",
    "        col=row+offset\n",
    "        dcm[row, col]=discount_rate**offset\n",
    "    return dcm\n",
    "  \n",
    "  def compute_rtgs(self):\n",
    "    dcm=torch.triu(torch.ones(self.len_traj, self.len_traj)).to(device)\n",
    "    rtgs=dcm.matmul(self.rewards)\n",
    "    self.rtgs=rtgs\n",
    "    return\n",
    "  \n",
    "  def compute_gaes(self, V_s, V_fs, lambd):\n",
    "    ts=torch.BoolTensor(self.termin_signals).float().to(device)\n",
    "    tdes=self.rewards+(1-ts)*V_fs-V_s\n",
    "    dcm=self.get_discounted_cumsum_matrix(self.len_traj, lambd) #undiscounted finite horizon setting\n",
    "    gaes=dcm.matmul(tdes)\n",
    "    return gaes\n",
    "\n",
    "  def finish_trajectory(self):\n",
    "    self.rewards=torch.FloatTensor(self.rewards).to(device)\n",
    "    self.action_probs=torch.FloatTensor(self.action_probs).to(device)\n",
    "    return\n",
    "\n",
    "  def add_ep_step(self, ep_step):\n",
    "    self.states.append(ep_step['state'])\n",
    "    self.state_prompts.append(ep_step['state_prompt'])\n",
    "    self.action_prompts_list.append(ep_step['action_prompts'])\n",
    "    self.action_indices.append(ep_step['action_index'])\n",
    "    self.action_probs.append(ep_step['action_prob'].item())\n",
    "    self.rewards.append(ep_step['reward'])\n",
    "    self.state_fs.append(ep_step['state_f'])\n",
    "    self.state_f_prompts.append(ep_step['state_f_prompt'])\n",
    "    self.action_f_prompts_list.append(ep_step['action_f_prompts'])\n",
    "    self.termin_signals.append(ep_step['termin_signal'])\n",
    "    self.categoricals.append(ep_step['categorical'])\n",
    "    self.masks.append(ep_step['mask'])\n",
    "    if ep_step['termin_signal']==True:\n",
    "      self.finish_trajectory()\n",
    "      self.len_traj=len(self.states)\n",
    "      #print(self.len_traj)\n",
    "      self.compute_rtgs()\n",
    "      self.finished=True\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used in training phase only\n",
    "class FlatTrajectory():\n",
    "  def __init__(self, batch_ep_steps_list):\n",
    "    #to store flattened episode steps\n",
    "    self.states=[]\n",
    "    self.state_prompts=[]\n",
    "    self.action_prompts_list=[]\n",
    "    self.action_indices=[]\n",
    "    self.action_probs=[]\n",
    "    self.rewards=[]\n",
    "    self.state_fs=[]\n",
    "    self.state_f_prompts=[]\n",
    "    self.action_f_prompts_list=[]\n",
    "    self.termin_signals=[]\n",
    "    self.masks=[]\n",
    "    self.categoricals=[]\n",
    "\n",
    "    self.len_trajs=[]\n",
    "    self.n_steps=0\n",
    "\n",
    "    #combine batch episode steps\n",
    "    self.batch_ep_steps_list=batch_ep_steps_list\n",
    "    self.trajectories=self.get_trajectories(batch_ep_steps_list)\n",
    "    self.get_flat_trajectory()\n",
    "\n",
    "  def get_discounted_cumsum_matrix(self, N, discount_rate):\n",
    "    dcm=torch.eye(N).to(device)\n",
    "    for offset in range(1,N):\n",
    "      max_row=N-offset\n",
    "      for row in range(max_row):\n",
    "        col=row+offset\n",
    "        dcm[row, col]=discount_rate**offset\n",
    "    return dcm\n",
    "\n",
    "  def compute_gaes(self, V_s, V_fs, lambd):\n",
    "    #discount cumsum matrix for gae computation: block diagonal\n",
    "    gae_dcm_list=[self.get_discounted_cumsum_matrix(len_traj, lambd) for len_traj in self.len_trajs]\n",
    "    gae_dcm=torch.block_diag(*gae_dcm_list)\n",
    "    termin_signal=torch.BoolTensor(self.termin_signals).float().to(device)\n",
    "    tdes=self.rewards+(1-termin_signal)*V_fs-V_s\n",
    "    gaes=torch.matmul(gae_dcm, tdes)\n",
    "    return gaes\n",
    "  \n",
    "  def compute_rtgs(self, dcm_list):\n",
    "    rtg_dcm=torch.block_diag(*dcm_list).to(device)\n",
    "    self.rtgs=torch.matmul(rtg_dcm, self.rewards)\n",
    "    return\n",
    "\n",
    "  def get_flat_trajectory(self):\n",
    "    dcm_list=[]\n",
    "    for trajectory in self.trajectories:\n",
    "      self.states.extend(trajectory.states)\n",
    "      self.state_prompts.extend(trajectory.state_prompts)\n",
    "      self.action_prompts_list.extend(trajectory.action_prompts_list)\n",
    "      self.action_indices.extend(trajectory.action_indices)\n",
    "      self.action_probs.append(trajectory.action_probs)\n",
    "      self.rewards.append(trajectory.rewards)\n",
    "      self.state_fs.extend(trajectory.state_fs)\n",
    "      self.state_f_prompts.extend(trajectory.state_f_prompts)\n",
    "      self.action_f_prompts_list.extend(trajectory.action_f_prompts_list)\n",
    "      self.termin_signals.extend(trajectory.termin_signals)\n",
    "      self.categoricals.extend(trajectory.categoricals)\n",
    "      self.masks.extend(trajectory.masks)\n",
    "      self.len_trajs.append(trajectory.len_traj)\n",
    "      dcm_list.append(torch.triu(torch.ones(trajectory.len_traj, trajectory.len_traj)).to(device))\n",
    "      #discount cumsum matrix for gae computation: block diagonal\n",
    "    self.rewards=torch.cat(self.rewards, dim=0).to(device)\n",
    "    self.action_probs=torch.cat(self.action_probs, dim=0).to(device)\n",
    "    self.compute_rtgs(dcm_list)\n",
    "    return\n",
    "  \n",
    "  def get_trajectories(self, batch_ep_steps_list):\n",
    "    batch_size=len(batch_ep_steps_list[0].states)\n",
    "    trajectories=[Trajectory() for _ in range(batch_size)]\n",
    "    for batch_ep_steps in batch_ep_steps_list:\n",
    "      for batch_idx in range(batch_size):\n",
    "        ep_step=batch_ep_steps[batch_idx]\n",
    "        #do not add dummy steps created during batch traj. generation\n",
    "        if ep_step['mask']==False:\n",
    "          trajectories[batch_idx].add_ep_step(ep_step)\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, N):\n",
    "    super(IndexDataset, self).__init__()\n",
    "    self.N=N\n",
    "    self.data=list(np.arange(0,self.N,1))\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.N\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using batch trajectories => memory error due to large-sized gradient tensors => compute with single trajectory.\n",
    "class PPO_RLQA_Trainer():\n",
    "  #targeted for hotpot data.\n",
    "  def __init__(self, hotpot_data, horizon):\n",
    "    self.agent=RLQA_Agent()\n",
    "    self.old_agent=RLQA_Agent()\n",
    "\n",
    "    #save data\n",
    "    self.train_data=hotpot_data['train']\n",
    "    self.val_data=hotpot_data['validation']\n",
    "    self.test_data=hotpot_data['test']\n",
    "\n",
    "    #create environments\n",
    "    self.train_env=HotPotQAEnv(self.train_data, train=True)\n",
    "    self.val_env=HotPotQAEnv(self.val_data, train=False)\n",
    "    self.test_env=HotPotQAEnv(self.test_data, train=False)\n",
    "\n",
    "    self.horizon=horizon\n",
    "\n",
    "    self.train_logs={\n",
    "        #joing loss\n",
    "        'loss_history': [],\n",
    "        #trajectory length during training\n",
    "        'len_traj_history': [],\n",
    "        #kl divergence\n",
    "        'kl_div_history': [],\n",
    "        'log10_kl_div_history': [],\n",
    "        #no. of iters per batch data\n",
    "        'avg_iter_history': [],\n",
    "        #time duration of operations\n",
    "        'traj_gen_time_history': [],\n",
    "        'iter_time_history': [],\n",
    "        #return and accuracy data\n",
    "        'train_return_history': [],\n",
    "        'train_acc_history': [],\n",
    "        'val_return_history': [],\n",
    "        'val_acc_history': [],\n",
    "        'best_val_return': 0,\n",
    "        'best_val_acc': 0,\n",
    "        'best_val_acc_epoch': 0,\n",
    "        #test data\n",
    "        'test_return': 0,\n",
    "        'test_acc': 0,\n",
    "    }\n",
    "\n",
    "    self.best_agent=RLQA_Agent()\n",
    "  \n",
    "  def plot_train_logs(self):\n",
    "    plt.figure(figsize=(25,25))\n",
    "    #plot policy loss\n",
    "    lh=self.train_logs['loss_history']\n",
    "    plt.subplot(3,3,1)\n",
    "    plt.plot(np.arange(1, len(lh)+1, 1), lh)\n",
    "    plt.title(\"Joint Loss History\")\n",
    "    plt.xlabel(\"Update Steps\")\n",
    "    plt.ylabel(\"Joint Loss\")\n",
    "    #plot value loss\n",
    "\n",
    "    #plot log-scaled kl divergence\n",
    "    plt.subplot(3,3,2)\n",
    "    klh=self.train_logs['log10_kl_div_history']\n",
    "    plt.plot(np.arange(1, len(klh)+1, 1), klh)\n",
    "    plt.title(\"Log-Scale KL Divergence History\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Log10 KL Divergence\")\n",
    "\n",
    "    #plot return history\n",
    "    plt.subplot(3,3,3)\n",
    "    trh=self.train_logs['train_return_history']\n",
    "    vrh=self.train_logs['val_return_history']\n",
    "    plt.plot(np.arange(1, len(trh)+1, 1), trh, 'b-', label='train avg return')\n",
    "    plt.plot(np.arange(1, len(vrh)+1, 1), vrh, \"r-\", label='validation avg return')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Average Return\")\n",
    "    plt.title(\"Average Return\")\n",
    "    plt.legend()\n",
    "\n",
    "    #plot accuracy history\n",
    "    plt.subplot(3,3,4)\n",
    "    tah=self.train_logs['train_acc_history']\n",
    "    vah=self.train_logs['val_acc_history']\n",
    "    plt.plot(np.arange(1, len(tah)+1, 1), tah, 'b-', label='train avg acc')\n",
    "    plt.plot(np.arange(1, len(vah)+1, 1), vah, \"r-\", label='validation avg acc')\n",
    "    plt.plot(self.train_logs['best_val_acc_epoch'], self.train_logs['best_val_acc'], \"go\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Average Accuracy\")\n",
    "    plt.title(\"Average Return\")\n",
    "    plt.legend()\n",
    "\n",
    "    #plot durations of operations\n",
    "    plt.subplot(3,3,5)\n",
    "    tgth=self.train_logs['traj_gen_time_history']\n",
    "    ith=self.train_logs['iter_time_history']\n",
    "    plt.plot(np.arange(1, len(tgth)+1, 1), tgth, 'b-', label='traj. gen. time')\n",
    "    plt.plot(np.arange(1, len(ith)+1, 1), ith, 'r-', label='iter time')\n",
    "    plt.xlabel(\"Batch Steps\")\n",
    "    plt.ylabel(\"Durations\")\n",
    "    plt.title(\"Operation Durations\")\n",
    "    plt.legend()\n",
    "\n",
    "    #plot average # of iterations over time\n",
    "    plt.subplot(3,3,6)\n",
    "    nih=self.train_logs['avg_iter_history']\n",
    "    plt.plot(np.arange(1, len(nih)+1, 1), nih)\n",
    "    plt.title(\"Loss Fitting Iters History\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"No. of iters\")\n",
    "\n",
    "    #plot average of trajectory length\n",
    "    plt.subplot(3,3,7)\n",
    "    lth=self.train_logs['len_traj_history']\n",
    "    plt.plot(np.arange(1, len(lth)+1, 1), lth)\n",
    "    plt.title(\"Avg. Traj. Length History(per batch)\")\n",
    "    plt.xlabel(\"Batch Steps\")\n",
    "    plt.ylabel(\"Avg. Traj. Length\")\n",
    "\n",
    "    plt.show()\n",
    "  \n",
    "  def load_model(self, name):\n",
    "    self.agent.load_state_dict(torch.load(os.path.join(base_dir, 'models/hotpot_ppo/{:s}.pt'.format(name))))\n",
    "    self.best_agent.load_state_dict(torch.load(os.path.join(base_dir, 'models/hotpot_ppo/{:s}.pt'.format(name))))\n",
    "    return\n",
    "\n",
    "  def save_best_model(self, num_epochs, batch_size, iters, lr, reg, lambd, eps, target_kl_div):\n",
    "    setting_string=str(num_epochs)+\"_\"+str(batch_size)+\"_\"+str(iters)+\"_\"+str(lr)+\"_\"+str(reg)+\"_\"+str(lambd)+\"_\"+str(eps)+\"_\"+str(target_kl_div)\n",
    "    model_dir=os.path.join(base_dir, 'models/hotpot_ppo/{:s}.pt'.format(setting_string))\n",
    "    torch.save(self.best_agent.state_dict(), model_dir)\n",
    "    return\n",
    "\n",
    "  def test(self):\n",
    "    print(\"Testing\")\n",
    "    batch_size=4\n",
    "    #run 10 episodes with best model, get avg. return\n",
    "    self.best_agent.eval()\n",
    "    index_dataset=IndexDataset(len(self.test_data))\n",
    "    index_loader=torch.utils.data.DataLoader(index_dataset, batch_size=batch_size, shuffle=True)\n",
    "    avg_return=0\n",
    "    avg_test_acc=0\n",
    "    test_pbar=tqdm(desc=\"testing\", total=len(self.test_data))\n",
    "    for b_idx, batch_indices in enumerate(index_loader):\n",
    "      _,_,batch_avg_return, batch_val_acc=self.generate_batch_trajectories(batch_indices, self.best_agent, self.test_env, temperature=0)\n",
    "      avg_return=avg_return+(batch_avg_return-avg_return)/(b_idx+1)\n",
    "      avg_test_acc=avg_test_acc+(batch_val_acc-avg_test_acc)/(b_idx+1)\n",
    "      test_pbar.update(batch_size)\n",
    "    test_pbar.close()\n",
    "    print(\"Average Test Return: {:.3f}, Average Test Accuracy: {:.3f}\".format(avg_return, avg_test_acc))\n",
    "    #log the results\n",
    "    self.train_logs['test_return']=avg_return\n",
    "    self.train_logs['test_acc']=avg_test_acc\n",
    "    return\n",
    "  \n",
    "  def validate(self):\n",
    "    #run 10 episodes with current model, get avg. return\n",
    "    batch_size=4\n",
    "    self.agent.eval()\n",
    "    index_dataset=IndexDataset(len(self.val_data))\n",
    "    index_loader=torch.utils.data.DataLoader(index_dataset, batch_size=batch_size, shuffle=True)\n",
    "    avg_return=0\n",
    "    avg_val_acc=0\n",
    "    val_pbar=tqdm(desc=\"validating\", total=len(self.val_data))\n",
    "    for b_idx, batch_indices in enumerate(index_loader):\n",
    "      _,_,batch_avg_return, batch_val_acc=self.generate_batch_trajectories(batch_indices, self.agent, self.val_env, temperature=0)\n",
    "      avg_return=avg_return+(batch_avg_return-avg_return)/(b_idx+1)\n",
    "      avg_val_acc=avg_val_acc+(batch_val_acc-avg_val_acc)/(b_idx+1)\n",
    "      val_pbar.update(batch_size)\n",
    "    val_pbar.close()\n",
    "    print(\"Average Val. Return: {:.3f}, Average Val. Accuracy: {:.3f}\".format(avg_return, avg_val_acc))\n",
    "    #log the results\n",
    "    self.train_logs['val_return_history'].append(avg_return)\n",
    "    self.train_logs['val_acc_history'].append(avg_val_acc)\n",
    "    if avg_val_acc>self.train_logs['best_val_acc']:\n",
    "      self.train_logs['best_val_acc']=avg_val_acc\n",
    "      self.train_logs['best_val_return']=avg_return\n",
    "      self.train_logs['best_val_acc_epoch']=len(self.train_logs['val_acc_history'])\n",
    "      self.best_agent=self.agent\n",
    "    return\n",
    "  \n",
    "  def get_trajectories(self, batch_ep_steps_list):\n",
    "    batch_size=len(batch_ep_steps_list[0].states)\n",
    "    trajectories=[Trajectory() for _ in range(batch_size)]\n",
    "    for batch_ep_steps in batch_ep_steps_list:\n",
    "      for batch_idx in range(batch_size):\n",
    "        ep_step=batch_ep_steps[batch_idx]\n",
    "        #do not add dummy steps created during batch traj. generation\n",
    "        if ep_step['mask']==False:\n",
    "          trajectories[batch_idx].add_ep_step(ep_step)\n",
    "    return trajectories\n",
    "  \n",
    "  def compute_kl_divergence(self, old_categoricals, new_categoricals):\n",
    "    #used for early stopping of fitting iterations.\n",
    "    #compute kl divergnece between current agent and old agent based on batch trajectories.\n",
    "    avg_kl_div=0\n",
    "    for idx, old_categorical in enumerate(old_categoricals):\n",
    "      new_categorical=new_categoricals[idx]\n",
    "      kl_div=torch.distributions.kl.kl_divergence(old_categorical, new_categorical).item()\n",
    "      avg_kl_div=avg_kl_div+(kl_div-avg_kl_div)/(idx+1)\n",
    "    return avg_kl_div\n",
    "  \n",
    "  #ftn.s for using separate trajectories\n",
    "  def generate_batch_trajectories(self, batch_indices, agent, env, temperature=1):\n",
    "    batch_size=len(batch_indices)\n",
    "    batch_states=env.batch_reset(batch_indices, self.horizon)\n",
    "    batch_termin_signals=[False for _ in range(batch_size)]\n",
    "    batch_traj_accs=[0 for _ in range(batch_size)]\n",
    "    batch_ep_steps_list=[]\n",
    "    #until all trajectories terminate\n",
    "    while sum(batch_termin_signals)!=batch_size:\n",
    "      #do not generate action for terminated tasks\n",
    "      mask=batch_termin_signals\n",
    "      #generate prompts only for \n",
    "      batch_state_prompts, batch_action_prompts_list=env.get_batch_actor_prompts(mask)\n",
    "\n",
    "      #categoricals used for KL-divergence computation\n",
    "      categoricals, action_indices, action_probs=agent(batch_state_prompts, batch_action_prompts_list, no_grad=True, temperature=temperature)\n",
    "    \n",
    "      batch_rewards, batch_state_fs, batch_termin_signals=env.batch_step(batch_size, action_indices, mask)\n",
    "      #print(action_indices, batch_rewards, batch_termin_signals)\n",
    "      batch_ep_steps=BatchEpisodeSteps(batch_states, action_indices, action_probs, batch_rewards, batch_state_fs, batch_termin_signals, categoricals, mask)\n",
    "      batch_ep_steps_list.append(batch_ep_steps)\n",
    "      #update state\n",
    "      batch_states=batch_state_fs\n",
    "    #get trajectories => using flat trajectory causes OUT OF MEMORY w/ Colab\n",
    "    trajectories=self.get_trajectories(batch_ep_steps_list)\n",
    "    #compute total # of steps inside batch trajectory\n",
    "    n_steps=sum([traj.len_traj for traj in trajectories])\n",
    "    #compute avg_return\n",
    "    batch_rewards=[traj.rewards for traj in trajectories]\n",
    "    reward_sums=[torch.sum(br).item() for br in batch_rewards]\n",
    "    avg_return=np.mean(reward_sums)\n",
    "    #compute accuracy\n",
    "    avg_acc=np.mean(env.batch_accuracies)\n",
    "    return trajectories, n_steps, avg_return, avg_acc\n",
    "\n",
    "  def get_joint_loss(self, batch_trajectories, lambd, eps):\n",
    "    #train actor and critic simultaneously.\n",
    "    #compute V_s, and V_fs \n",
    "    avg_joint_loss=torch.FloatTensor([0]).to(device)\n",
    "    avg_kl_div=0\n",
    "    for t_idx, trajectory in enumerate(batch_trajectories):\n",
    "      V_s=self.agent.critic(trajectory.state_prompts)\n",
    "      V_fs=self.agent.critic(trajectory.state_f_prompts)\n",
    "\n",
    "      #compute GAEs\n",
    "      gaes=trajectory.compute_gaes(V_s, V_fs, lambd)\n",
    "      #get action probs w.r.t current policy\n",
    "      old_action_probs=trajectory.action_probs\n",
    "      old_categoricals=trajectory.categoricals\n",
    "      new_categoricals, new_action_probs=self.agent.actor.get_action_probs(trajectory.state_prompts, \n",
    "                                                        trajectory.action_prompts_list, trajectory.action_indices)\n",
    "      #compute kl div\n",
    "      kl_div=self.compute_kl_divergence(old_categoricals, new_categoricals)\n",
    "      avg_kl_div=avg_kl_div+(kl_div-avg_kl_div)/(t_idx+1)\n",
    "\n",
    "      #get policy loss\n",
    "      prob_ratios=torch.div(new_action_probs, old_action_probs)\n",
    "      first_term=torch.mul(prob_ratios, gaes)\n",
    "      second_term=torch.mul(torch.clamp(prob_ratios, 1-eps, 1+eps), gaes)\n",
    "      policy_loss=-torch.mean(torch.minimum(first_term, second_term))\n",
    "\n",
    "      #get value loss\n",
    "      mse_loss=nn.MSELoss()\n",
    "      value_loss=0.5*mse_loss(trajectory.rtgs, V_s)\n",
    "      joint_loss=policy_loss+value_loss\n",
    "      avg_joint_loss=avg_joint_loss+(joint_loss-avg_joint_loss)/(t_idx+1)\n",
    "\n",
    "    #log scale of KL divergence\n",
    "    if avg_kl_div>0:\n",
    "      log10_avg_kl_div=np.log(avg_kl_div)\n",
    "    else:\n",
    "      log10_avg_kl_div=np.log(-avg_kl_div)\n",
    "      print(avg_kl_div)\n",
    "    return avg_kl_div, log10_avg_kl_div, avg_joint_loss\n",
    "    \n",
    "  def train(self, num_epochs, batch_size, iters, lr, reg, lambd, eps, target_kl_div):\n",
    "    total_steps=num_epochs*len(self.train_data)\n",
    "    optimizer=optim.AdamW(self.agent.parameters(), lr=lr, weight_decay=reg)\n",
    "    #scheduler=transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=int(0.2*total_steps), num_training_steps=total_steps)\n",
    "    scheduler=transformers.get_constant_schedule_with_warmup(optimizer, num_warmup_steps=int(0.2*total_steps))\n",
    "    #run PPO\n",
    "    pbar=tqdm(desc=\"PPO training\", total=num_epochs*len(self.train_data))\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "      avg_iter=0\n",
    "      #single epoch\n",
    "      index_dataset=IndexDataset(len(self.train_data))\n",
    "      index_loader=torch.utils.data.DataLoader(index_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "      #measuring average train trajectories' return and accuracy\n",
    "      avg_train_acc=0\n",
    "      avg_train_return=0\n",
    "      avg_len_traj=0\n",
    "      for b_idx, batch_indices in enumerate(index_loader):\n",
    "        traj_gen_start=time.time()\n",
    "        batch_trajectories, n_steps, batch_train_return, batch_train_acc=self.generate_batch_trajectories(batch_indices, self.old_agent, self.train_env)\n",
    "        #log the average length of trajectory on each batch\n",
    "        self.train_logs['len_traj_history'].append(n_steps/batch_size)\n",
    "        traj_gen_end=time.time()\n",
    "        traj_gen_time=traj_gen_end-traj_gen_start\n",
    "        #log duartion of trajectory generation\n",
    "        self.train_logs['traj_gen_time_history'].append(traj_gen_time)\n",
    "        #print(\"Traj Gen Duration for epoch={:d}, batch idx={:d}: {:.5f}\".format(epoch, b_idx, traj_gen_time))\n",
    "\n",
    "        #policy gradient + critic loss joint fitting\n",
    "        total_iter_time=0\n",
    "        for iter in range(iters):\n",
    "          iter_start=time.time()\n",
    "          kl_div, log10_kl_div, joint_loss=self.get_joint_loss(batch_trajectories, lambd, eps)\n",
    "          #print(\"Log10 KL div: {:s},  Loss: {:.3f}\".format(str(log10_kl_div), joint_loss.item()))\n",
    "          self.train_logs['loss_history'].append(joint_loss.item())\n",
    "          self.train_logs['kl_div_history'].append(kl_div)\n",
    "          self.train_logs['log10_kl_div_history'].append(log10_kl_div)\n",
    "\n",
    "          #during fitting iteration: should employ KL-divergence based early stopping: preventing agent's policy moving too far away from old policy\n",
    "          if kl_div>target_kl_div:\n",
    "            #record average iter\n",
    "            avg_iter=avg_iter+(iter-avg_iter)/(b_idx+1)\n",
    "            break\n",
    "          \n",
    "          optimizer.zero_grad()\n",
    "          joint_loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          #ending loss-fitting iteration\n",
    "          iter_end=time.time()\n",
    "          iter_time=iter_end-iter_start\n",
    "          total_iter_time+=iter_time\n",
    "          #print(\"Single Iter Duration for epoch={:d}, batch idx={:d}, iter idx={:d}: {:.5f}\".format(epoch, b_idx, iter, iter_time))\n",
    "          #print(\"-------\")\n",
    "\n",
    "          #record avg. iter if all iters are used w/o violating target KL Divergence\n",
    "          if iter==iters-1:\n",
    "            avg_iter=avg_iter+(iter-avg_iter)/(b_idx+1)\n",
    "        \n",
    "        #log duration of loss fitting iterations.\n",
    "        self.train_logs['iter_time_history'].append(total_iter_time)\n",
    "\n",
    "        #replace old agent's weight with current agent's weight\n",
    "        self.old_agent.load_state_dict(self.agent.state_dict())\n",
    "        #incremental update on train average metrics.\n",
    "        avg_train_acc=avg_train_acc+(batch_train_acc-avg_train_acc)/(b_idx+1)\n",
    "        avg_train_return=avg_train_return+(batch_train_return-avg_train_return)/(b_idx+1)\n",
    "        pbar.update(batch_size)\n",
    "        #step scheduler\n",
    "        scheduler.step()\n",
    "      \n",
    "      #end of epoch\n",
    "      #validate, add average train acc, and avg. no. of loss fitting iters\n",
    "      self.train_logs['train_acc_history'].append(avg_train_acc)\n",
    "      self.train_logs['train_return_history'].append(avg_train_return)\n",
    "      self.train_logs['avg_iter_history'].append(avg_iter)\n",
    "      self.validate()\n",
    "      \n",
    "    #final testing\n",
    "    self.test()\n",
    "    self.save_best_model(num_epochs, batch_size, iters, lr, reg, lambd, eps, target_kl_div)\n",
    "    return self.train_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLQA_tuner():\n",
    "  def __init__(self, hotpot_data, horizon, start_model_name=None):\n",
    "    self.hotpot_data=hotpot_data\n",
    "    self.horizon=horizon\n",
    "    self.start_model_name=start_model_name\n",
    "\n",
    "    self.tuning_logs={\n",
    "        'logs': {},\n",
    "        'best_setting': [],\n",
    "        'best_test_return': 0,\n",
    "        'best_test_acc': 0,\n",
    "    }\n",
    "\n",
    "    self.current_trainer=None\n",
    "    \n",
    "    self.best_agent=RLQA_Agent()\n",
    "  \n",
    "  def save_results(self, model_name, log_name):\n",
    "    #save model\n",
    "    model_dir=os.path.join(base_dir, 'models/hotpot_ppo/{:s}.pt'.format(model_name))\n",
    "    torch.save(self.best_agent.state_dict(), model_dir)\n",
    "    #save tuning logs\n",
    "    with open(os.path.join(base_dir, 'experiment logs/hotpot_ppo/{:s}.pkl'.format(log_name)), 'wb') as file:\n",
    "      pickle.dump(self.tuning_logs, file)\n",
    "    return\n",
    "  \n",
    "  def get_settings(self, num_epochs_list, batch_size_list, iters_list, lr_list, reg_list, lambd_list, eps_list, target_kl_div_list):\n",
    "    settings=[]\n",
    "    for num_epochs in num_epochs_list:\n",
    "      for batch_size in batch_size_list:\n",
    "        for iters in iters_list:\n",
    "          for lr in lr_list:\n",
    "            for reg in reg_list:\n",
    "              for lambd in lambd_list:\n",
    "                for eps in eps_list:\n",
    "                  for target_kl_div in target_kl_div_list:\n",
    "                    setting=(num_epochs, batch_size, iters, lr, reg, lambd, eps, target_kl_div)\n",
    "                    settings.append(setting)\n",
    "    return settings\n",
    "  \n",
    "  def show_setting(self, setting):\n",
    "    print(\"num_epochs={:d}, batch_size={:d}, iters={:d}, lr={:s}, reg={:s}, lambd={:.2f}, eps={:.2f}, target_kl_div={:s}\".format(setting[0], setting[1], setting[2], str(setting[3]), str(setting[4]), setting[5], setting[6], str(setting[7])))\n",
    "    return\n",
    "  \n",
    "  def tune(self, num_epochs_list, batch_size_list, iters_list, lr_list, reg_list, lambd_list, eps_list, target_kl_div_list):\n",
    "    settings=self.get_settings(num_epochs_list, batch_size_list, iters_list, lr_list, reg_list, lambd_list, eps_list, target_kl_div_list)\n",
    "    outer_pbar=tqdm(desc=\"PPO RLQA Tuning\", total=len(settings))\n",
    "    for setting in settings:\n",
    "      self.show_setting(setting)\n",
    "      self.current_trainer=PPO_RLQA_Trainer(self.hotpot_data, self.horizon)\n",
    "      if self.start_model_name!=None:\n",
    "        self.current_trainer.load_model(self.start_model_name)\n",
    "      train_logs=self.current_trainer.train(*setting)\n",
    "      self.current_trainer.plot_train_logs()\n",
    "      #add to logs\n",
    "      self.tuning_logs['logs'][setting]=train_logs\n",
    "\n",
    "      #update best model\n",
    "      if train_logs['test_acc']>self.tuning_logs['best_test_acc']:\n",
    "        self.tuning_logs['best_test_return']=train_logs['test_return']\n",
    "        self.tuning_logs['best_test_acc']=train_logs['test_acc']\n",
    "        self.tuning_logs['best_setting']=setting\n",
    "        self.best_agent=self.current_trainer.best_agent\n",
    "\n",
    "      outer_pbar.update(1)\n",
    "    outer_pbar.close()\n",
    "    return self.tuning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_list=[5]\n",
    "batch_size_list=[4] \n",
    "iters_list=[5] \n",
    "lr_list=[5e-5] \n",
    "reg_list=[0]\n",
    "lambd_list=[0.97]\n",
    "eps_list=[0.2]\n",
    "target_kl_div_list=[0.03] #generally 0.003 ~ 0.03\n",
    "tuner=RLQA_tuner(processed_hotpot_data, horizon=60) \n",
    "#set gamma to 1 by default (undiscounted finite horizon setting)\n",
    "tuning_logs=tuner.tune(num_epochs_list, batch_size_list, iters_list, lr_list, reg_list, lambd_list, eps_list, target_kl_div_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "641a7458bfae2bc959d7f867e9e3882167acabe29543290f7c5231fa0d54378e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
